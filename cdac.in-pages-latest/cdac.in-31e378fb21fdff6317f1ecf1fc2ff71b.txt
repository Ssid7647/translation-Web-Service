#URL => https://www.cdac.in/index.aspx?id=ev_hpc_hadoop-itrtv-map-reduce
Abcd-2014
Reach Us
Skip to navigation
Skip to main content
C-DAC Centres
Bengaluru
Chennai
Delhi
Hyderabad
Kolkata
Mohali
Mumbai
Noida
Patna
Pune
Silchar
Thiruvananthapuram
Sitemap
Blog
Choose Language
Assamese
Bangla
Bodo
Dogri
Gujarati
Kannada
Konkani
Kashmiri
Kashmiri-Keshur
Maithili
Malayalam
Manipuri
Manipuri (N)
Marathi
Nepali
Oriya
Punjabi
Santali
Santali (N)
Sanskrit
Sindhi
Sindhi (N)
Tamil
Telugu
Urdu
Translation powered by GoTranslate
Regional Language Policy
A-
A+
Toggle navigation
Home
About C-DAC (current)
Products & Services
Research & Development
Press Kit
Downloads
Careers
Tenders
Contact Us
Search
Events
Topics
Tech Program
Speakers
Lab
Registration
Accomodation
Venue
ABCD-2014 : Hadoop MapReduce
The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.
It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.
Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.
This section contains simple hadoop MapReduce examples of wordcount, matrix multiplication, finding maximum temperature, converting small image files to sequential files.
That will provide basic idea, how to use MapReduce programing framework in hadoop cluster and about some basic APIs required to write Hadoop MapReduce programe and how key-value mapping done to get desire output
Hadoop MapReduce :     Compiliation & Execution :  Command Line  run_program.sh )
view source
print
Example 1.1
Write a Hadoop MapReduce program for breadth-first search (BFS) using Hadoop MapReduce Framework
( Download source code : : bfs-iterative-map-reduce (WinRAR ZIP archive)  )
( Download Description of Problem (PDF) : BFS-Iterative MapReduce -PDF file)  )
Example 1.2
(Assignment)
Write Iterative MapReduce Program for Frequent Sub-graph Mining
Subgraph-Mining-MapReduce-PDF file)  )
Compilation and Execution
Pre-requisites for Hadoop programs Compilation :
JavaTM 1.5.x, preferably from Sun, must be installed.
ssh must be installed and  sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons.
Hadoop-1.1.x or 2.2.x must be installed.
Commands for Verification of Hadoop setup :
$hadoop version
Hadoop 2.2.0 Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768 Compiled by hortonmu on 2013-10-07T06:28Z Compiled with protoc 2.5.0 From source with checksum 79e53ce7994d1628b240f09af91e1af4...
$jps
7814 DataNode
8288 NodeManager
15704 Jps
7667 NameNode
8170 ResourceManager
7991 SecondaryNameNode
STEP-1 :
For compilation and execution of MapReduce programs we require Java environment (JVM) and hadoop-2.2.0-core.jar (from Hadoop-1.1.x or 2.2.x)
For compilation of code use following commands
$ javac -classpath < path to hadoop-2.2-core.jar> -d wordcount_classes WordCount.java
where wordcount_classes is a folder that contains all .class file generated from this compilation
STEP-2 :
Converting all 'class' files (created in previous STEP-1) into 'jar' file
For creation of jar file use following commands
$ jar -cvf wordcount.jar -C wordcount_classes/ .
where wordcount.jar is the jar file created after this command execution and serves as executable jar for hadoop.
STEP-3 :
After creating 'jar' file, copy it into HDFS by using following command :
$ hadoop fs -copyFromLocal input.txt wordcount_input
where wordcount_input is the input file created in HDFS that contain your input data.
STEP-4 :
Launching hadoop executable to execute 'jar' file with input file
$ hadoop jar wordcount.jar WordCount wordcount_input wordcount_output
where WordCount is the driver class in wordcount.jar and wordcount_output is the output folder created in HDFS that contain two files
_SUCCESS which is basically a success log file
part-r-00000 which is output file generated by hadoop framework.
For output (part-r-00000) that present in HDFS execute following command.
$ hadoop fs -cat wordcount_output/part-r-00000
For copying file from HDFS to local Filesystem execute following command.
$ hadoop fs -copyToLocal wordcount_output/part-r-00000 output.txt
where output.txt is the output file created in current directory that contain actual output data.
High Performance Computing,
Grid & Cloud Computing
Multilingual Computing & Heritage Computing
Professional Electronics,
VLSI & Embedded Systems
Software Technologies including FOSS
Cyber Security & Cyber Forensics
Health Informatics
Education & Training
Related Links
Office Contact Information
Career Opportunities
Website Policies
Copyright Policy
Terms & Conditions
Help
Â© 2021
C-DAC.
All rights reserved
Last Updated: Tuesday, January 30, 2018
Website owned & maintained by: Centre for Development of Advanced Computing (C-DAC)
C-DAC LOGO
Manipuri(N)
Santali(N)
Sindhi(N)
dbg
lbg
copy to clipboard
?
MEITY
Digital India
Azadi Ka Amrit Mahotsav
India.gov
BHIM
Swachh Bharat
Koo
Facebook
linkedin
twitter
